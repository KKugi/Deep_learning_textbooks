{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP+ram9egvWhQma+/65ub6B"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"yZ5jHRKr8KUy"},"outputs":[],"source":["import torch\n","import matplotlib.pyplot as plt\n","\n","x = torch.tensor([150, 160, 170, 175, 185.])\n","y = torch.tensor([55, 70, 64, 80, 75.])\n","N = len(x)\n","\n","plt.plot(x,y,'o')"]},{"cell_type":"code","source":["# 초기값 설정\n","a = 0.45\n","b = -35\n","\n","x_plot = torch.linspace(145,190,100)\n","y_plot = a * x_plot + b\n","\n","plt.plot(x,y,'o')\n","plt.plot(x_plot, y_plot, 'r')"],"metadata":{"id":"cH0KXRbA8Qbk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# a,b 를 바꿔가면서 Loss 값을 일일히 구해서 가장 작아지게 하는 a,b를 선정\n","\n","a = 0.5 + torch.linspace(-0.2, 0.2, 100) # 100개의 값 (기울기 후보)\n","b = -30 + torch.linspace(-20, 20, 100) # 100개의 값 (절편 후보)\n","\n","L=torch.zeros(len(b), len(a))\n","\n","for i in range(len(b)):\n","    for j in range(len(a)):\n","        for n in range(N):\n","            L[i,j] = L[i,j] + (y[n] - (a[j]*x[n]+b[i])) ** 2\n","\n","L = L/N # MSE\n","print(L)\n","\n","plt.figure(figsize=[10,9])\n","ax = plt.axes(projection='3d')\n","A, B = torch.meshgrid(a,b)\n","print(A)\n","print(B)\n","ax.plot_surface(A,B,L)\n","ax.set_xlabel('a')\n","ax.set_ylabel('b')\n","ax.set_zlim([0,1000])\n","\n","plt.figure()\n","plt.contour(a,b,L,30)\n","plt.xlabel('a')\n","plt.ylabel('b')\n","plt.grid()"],"metadata":{"id":"teW29IHY9P0B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 행으로는 기울기\n","# 열로는 y 절편을 바꿔가면서 loss 값을 얻음\n","\n","print(L)\n","print(torch.min(L))\n","\n","# print(A)\n","# print(B)\n","\n","a_opt = A[L==torch.min(L)]\n","print(a_opt)\n","b_opt = B[L==torch.min(L)]\n","print(b_opt)\n","\n","min_val = torch.min(L)\n","min_idx = torch.nonzero(L == min_val) # torch.nonzero는 텐서에서 값이 0이 아닌 원소(=True인 원소)의 위치(인덱스)를 찾아서 반환\n","print(min_idx)\n","\n","i, j = min_idx[0]\n","print(j)\n","print(f\"i = {i.item()}, j = {j.item()}\")\n","\n","a_opt = A[i, j]\n","b_opt = B[i, j]\n","print(f\"기울기 a_opt = {a_opt.item()}\")\n","print(f\"절편 b_opt = {b_opt.item()}\")"],"metadata":{"id":"sv1xxLSJFHwa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_plot=torch.linspace(145,190,100)\n","# y=ax+b : a(기울기), b(절편)\n","# y=0.5424*x+(-23.7374)\n","y_plot=a_opt * x_plot + b_opt\n","\n","plt.plot(x, y, 'o')\n","plt.plot(x_plot, y_plot, 'r')"],"metadata":{"id":"jXu12Si1KGry"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import matplotlib.pyplot as plt\n","\n","x = torch.tensor([150, 160, 170, 175, 185.])\n","y = torch.tensor([55, 70, 64, 80, 75.])\n","N = len(x)\n","\n","# 모델 파라미터 초기화\n","a = torch.tensor([.45], requires_grad=True)\n","b = torch.tensor([-35.], requires_grad=True)\n","\n","# 하이퍼파라미터 설정\n","LR = 3e-6\n","EPOCH = 20\n","\n","loss_history = []\n","\n","for ep in range(EPOCH):\n","    # inference\n","    y_hat = a * x + b\n","    # print(y_hat)\n","\n","    # loss 계산\n","    loss = 0\n","    for n in range(N):\n","        loss += (y[n] - y_hat[n]) ** 2\n","\n","    loss = loss/N # MSE\n","\n","    # gradient descent (가중치 update)\n","    loss.backward() # backpropagation : 손실에 대해 a, b 에 대한 gradient 계산(손실을 a와 b에 대해 각각 미분한 값 계산)\n","\n","    with torch.no_grad():\n","        \"\"\"\n","        autograd 추척 비활성화 : 해당 블록 내부의 연산은 미분하지 않음\n","        why?\n","        파라미터 업데이트는 이미 계산된 gradient(기울기)를 이용하는 것이지,\n","        업데이트 연산 자체에 대해 또 미분할 필요가 없기 때문\n","        \"\"\"\n","        a -= LR * a.grad # weight update\n","        b -= LR * b.grad # weight update\n","\n","    # gradient 초기화 (다음 epoch을 위해 필수)\n","    \"\"\"\n","    PyTorch의 gradient 처리: 기본적으로 누적됨\n","    초기화 안 하면?: 이전 epoch의 gradient가 다음에도 영향을 미쳐 학습이 꼬임\n","    왜 초기화?: 매 epoch마다 새롭게 계산된 gradient만 반영하기 위해\n","    \"\"\"\n","    a.grad = torch.tensor([0.]) # 혹은 a.arad.zero(), 누적된 a의 gradient 초기화\n","    b.grad = torch.tensor([0.]) # 혹은 b.arad.zero(), 누적된 b의 gradient 초기화\n","\n","    # print loss\n","    loss_history += [loss.item()]\n","    print(f\"Epoch: {ep+1}, train loss: {loss.item():.4f}\")\n","\n"],"metadata":{"id":"9Qw1mFi4KwEN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sQ4o0c2CNVLm"},"execution_count":null,"outputs":[]}]}